{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converged Action Grid:\n",
      " [[1. 1. 1. 1. 2.]\n",
      " [2. 2. 2. 2. 2.]\n",
      " [2. 1. 2. 3. 2.]\n",
      " [1. 1. 0. 3. 2.]\n",
      " [1. 1. 0. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "R = 5\n",
    "C = 5\n",
    "A = 5\n",
    "\n",
    "N = R * C\n",
    "\n",
    "epsilon = 0.1\n",
    "# epsilon = 0.0\n",
    "\n",
    "action_grid = [\n",
    "[1, 1, 1, 1, 1],\n",
    "[1, 1, 1, 1, 1],\n",
    "[1, 1, 1, 1, 1],\n",
    "[1, 1, 1, 1, 1],\n",
    "[1, 1, 1, 1, 1],\n",
    "]\n",
    "# action_grid = (np.random.rand(R, C) * A).astype(int)\n",
    "\n",
    "# Policy Matrix.\n",
    "# pi[i][j] mean the probability of `at state i choose action j`.\n",
    "pi = np.zeros((N, A))\n",
    "for r_ in range(R):\n",
    "    for c_ in range(C):\n",
    "        pi[r_ * C + c_] = 1 / A\n",
    "\n",
    "# Block states.\n",
    "blocks = np.array([\n",
    "[-1,  -1,  -1,  -1, -1],\n",
    "[-1, -10, -10,  -1, -1],\n",
    "[-1,  -1, -10,  -1, -1],\n",
    "[-1, -10,   0, -10, -1],\n",
    "[-1, -10,  -1,  -1, -1],\n",
    "])\n",
    "# blocks = np.array([\n",
    "# [0,   0,   0,   0,  0],\n",
    "# [0, -10, -10,   0,  0],\n",
    "# [0,   0, -10,   0,  0],\n",
    "# [0, -10,   1, -10,  0],\n",
    "# [0, -10,   0,   0,  0],\n",
    "# ])\n",
    "\n",
    "action_to_direction = {}\n",
    "action_to_direction[0] = np.array([-1, 0])\n",
    "action_to_direction[1] = np.array([0, 1])\n",
    "action_to_direction[2] = np.array([1, 0])\n",
    "action_to_direction[3] = np.array([0, -1])\n",
    "action_to_direction[4] = np.array([0, 0])\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "# State Values.\n",
    "v = np.ones((R * C, 1))\n",
    "\n",
    "# Episode Length.\n",
    "# If not decrease epsilon, this E should be large for convergence.\n",
    "E = 100\n",
    "\n",
    "q_table = np.zeros((N, A))\n",
    "\n",
    "num_episode = 300\n",
    "\n",
    "alpha = 0.1\n",
    "r_start = 0\n",
    "c_start = 0\n",
    "r_target = 3\n",
    "c_target = 2\n",
    "\n",
    "for episode in range(num_episode):\n",
    "    # 1. Random start.\n",
    "    state = (r_start, c_start)\n",
    "    a_start = np.random.choice(range(A), size=1, replace=False, p=pi[r_start * C + c_start])[0]\n",
    "\n",
    "    # 2. Generate a Episode.\n",
    "    episode_history = []\n",
    "    for e in range(E):\n",
    "        r_, c_ = state\n",
    "        a = a_start\n",
    "        if e != 0:\n",
    "            a = np.random.choice(range(A), size=1, replace=False, p=pi[r_ * C + c_])[0]\n",
    "\n",
    "        delta = action_to_direction[a]\n",
    "        r_p = r_ + delta[0]\n",
    "        c_p = c_ + delta[1]\n",
    "\n",
    "        if r_p >= R or r_p < 0 or c_p >= C or c_p < 0:\n",
    "            reward = -10\n",
    "            next_state = (r_, c_)\n",
    "        else:\n",
    "            reward = blocks[r_p][c_p]\n",
    "            next_state = (r_p, c_p)\n",
    "\n",
    "        episode_history.append((state, a, reward))\n",
    "        state = next_state\n",
    "\n",
    "        if next_state[0] == r_target and next_state[1] == c_target:\n",
    "            break\n",
    "    \n",
    "    for e in range(len(episode_history) - 1):\n",
    "        (r_, c_), a, reward = episode_history[e]\n",
    "        (r_next, c_next), a_next, _ = episode_history[e + 1]\n",
    "        # Update q-value.\n",
    "        TD_target = reward + gamma * q_table[r_next * C + c_next][a_next]\n",
    "        TD_error = q_table[r_ * C + c_][a] - TD_target\n",
    "        q_table[r_ * C + c_][a] -= alpha * TD_error\n",
    "        # Update Policy.\n",
    "        max_a = np.argmax(q_table[r_ * C + c_])\n",
    "        pi[r_ * C + c_] = epsilon / A\n",
    "        pi[r_ * C + c_][max_a] = 1 - epsilon / A * (A - 1)\n",
    "\n",
    "    ## Enhance Exploitation.\n",
    "    if epsilon > 0.001:\n",
    "        epsilon -= 0.001\n",
    "\n",
    "    time.sleep(0.2)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    print('len(episode_history):', len(episode_history))\n",
    "    action_grid = np.zeros((R, C))\n",
    "    for r_ in range(R):\n",
    "        for c_ in range(C):\n",
    "            a = np.argmax(pi[r_ * C + c_])\n",
    "            action_grid[r_][c_] = a\n",
    "    print(f'{episode}th Action Grid:\\n', action_grid)\n",
    "\n",
    "clear_output(wait=True)\n",
    "\n",
    "action_grid = np.zeros((R, C))\n",
    "for r_ in range(R):\n",
    "    for c_ in range(C):\n",
    "        a = np.argmax(pi[r_ * C + c_])\n",
    "        action_grid[r_][c_] = a\n",
    "print(f'Converged Action Grid:\\n', action_grid, flush=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
