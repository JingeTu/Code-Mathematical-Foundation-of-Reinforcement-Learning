{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged Action Grid:\n",
      " [[1. 1. 1. 2. 2.]\n",
      " [2. 2. 2. 2. 3.]\n",
      " [2. 1. 2. 1. 2.]\n",
      " [1. 1. 0. 3. 2.]\n",
      " [4. 1. 0. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "R = 5\n",
    "C = 5\n",
    "A = 5\n",
    "\n",
    "N = R * C\n",
    "\n",
    "epsilon = 0.1\n",
    "# epsilon = 0.0\n",
    "\n",
    "action_grid = [\n",
    "[1, 1, 1, 1, 1],\n",
    "[1, 1, 1, 1, 1],\n",
    "[1, 1, 1, 1, 1],\n",
    "[1, 1, 1, 1, 1],\n",
    "[1, 1, 1, 1, 1],\n",
    "]\n",
    "# action_grid = (np.random.rand(R, C) * A).astype(int)\n",
    "\n",
    "# Policy Matrix.\n",
    "# pi[i][j] mean the probability of `at state i choose action j`.\n",
    "pi = np.zeros((N, A))\n",
    "for r_ in range(R):\n",
    "    for c_ in range(C):\n",
    "        pi[r_ * C + c_] = 1 / A\n",
    "\n",
    "# Block states.\n",
    "blocks = np.array([\n",
    "[-1,  -1,  -1,  -1, -1],\n",
    "[-1, -10, -10,  -1, -1],\n",
    "[-1,  -1, -10,  -1, -1],\n",
    "[-1, -10,   0, -10, -1],\n",
    "[-1, -10,  -1,  -1, -1],\n",
    "])\n",
    "# blocks = np.array([\n",
    "# [0,   0,   0,   0,  0],\n",
    "# [0, -10, -10,   0,  0],\n",
    "# [0,   0, -10,   0,  0],\n",
    "# [0, -10,   1, -10,  0],\n",
    "# [0, -10,   0,   0,  0],\n",
    "# ])\n",
    "\n",
    "action_to_direction = {}\n",
    "action_to_direction[0] = np.array([-1, 0])\n",
    "action_to_direction[1] = np.array([0, 1])\n",
    "action_to_direction[2] = np.array([1, 0])\n",
    "action_to_direction[3] = np.array([0, -1])\n",
    "action_to_direction[4] = np.array([0, 0])\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "# State Values.\n",
    "v = np.ones((R * C, 1))\n",
    "\n",
    "# Episode Length.\n",
    "# If not decrease epsilon, this E should be large for convergence.\n",
    "E = 100\n",
    "\n",
    "# Sarsa step.\n",
    "sarsa_n = 3\n",
    "\n",
    "q_table = np.zeros((N, A))\n",
    "\n",
    "num_episode = 300\n",
    "\n",
    "alpha = 0.1\n",
    "r_start = 0\n",
    "c_start = 0\n",
    "r_target = 3\n",
    "c_target = 2\n",
    "\n",
    "for episode in range(num_episode):\n",
    "    # 1. Random start.\n",
    "    state = (r_start, c_start)\n",
    "    a_start = np.random.choice(range(A), size=1, replace=False, p=pi[r_start * C + c_start])[0]\n",
    "\n",
    "    # 2. Generate a Episode.\n",
    "    episode_history = []\n",
    "    for e in range(E):\n",
    "        r_, c_ = state\n",
    "        a = a_start\n",
    "        if e != 0:\n",
    "            a = np.random.choice(range(A), size=1, replace=False, p=pi[r_ * C + c_])[0]\n",
    "\n",
    "        delta = action_to_direction[a]\n",
    "        r_p = r_ + delta[0]\n",
    "        c_p = c_ + delta[1]\n",
    "\n",
    "        if r_p >= R or r_p < 0 or c_p >= C or c_p < 0:\n",
    "            reward = -10\n",
    "            next_state = (r_, c_)\n",
    "        else:\n",
    "            reward = blocks[r_p][c_p]\n",
    "            next_state = (r_p, c_p)\n",
    "\n",
    "        # # 1. Naive Sarsa.\n",
    "        # if len(episode_history) > 0:\n",
    "        #     (r_last, c_last), a_last, reward_last = episode_history[-1]\n",
    "        #     TD_target = reward_last + gamma * q_table[r_ * C + c_][a]\n",
    "        #     TD_error = q_table[r_last * C + c_last][a_last] - TD_target\n",
    "        #     q_table[r_last * C + c_last][a_last] -= alpha * TD_error\n",
    "        #     # Update Policy.\n",
    "        #     max_a = np.argmax(q_table[r_last * C + c_last])\n",
    "        #     pi[r_last * C + c_last] = epsilon / A\n",
    "        #     pi[r_last * C + c_last][max_a] = 1 - epsilon / A * (A - 1)\n",
    "\n",
    "\n",
    "        # # 2. Expected Sarsa.\n",
    "        # r_next, c_next = next_state\n",
    "        # # Update q-value.\n",
    "        # TD_target = reward\n",
    "        # for a_ in range(A):\n",
    "        #     TD_target += gamma * pi[r_next * C + c_next][a_] * q_table[r_next * C + c_next][a_]\n",
    "        # TD_error = q_table[r_ * C + c_][a] - TD_target\n",
    "        # q_table[r_ * C + c_][a] -= alpha * TD_error\n",
    "        # # Update Policy.\n",
    "        # max_a = np.argmax(q_table[r_ * C + c_])\n",
    "        # pi[r_ * C + c_] = epsilon / A\n",
    "        # pi[r_ * C + c_][max_a] = 1 - epsilon / A * (A - 1)\n",
    "\n",
    "        # 3. n-step Sarsa.\n",
    "        if len(episode_history) > sarsa_n - 1:\n",
    "            head_idx = -(sarsa_n)\n",
    "            (r_head, c_head), a_head, reward_head = episode_history[head_idx]\n",
    "            TD_target = reward_head\n",
    "            g = gamma\n",
    "            for i in range(1, sarsa_n + 1):\n",
    "                (r_last, c_last), a_last, reward_last = episode_history[head_idx + i]\n",
    "                if i != sarsa_n:\n",
    "                    TD_target += g * reward_last\n",
    "                else:\n",
    "                    TD_target += g * q_table[r_ * C + c_][a]\n",
    "                g *= gamma\n",
    "            TD_error = q_table[r_head * C + c_head][a_head] - TD_target\n",
    "            q_table[r_head * C + c_head][a_head] -= alpha * TD_error\n",
    "            # Update Policy.\n",
    "            max_a = np.argmax(q_table[r_head * C + c_head])\n",
    "            pi[r_head * C + c_head] = epsilon / A\n",
    "            pi[r_head * C + c_head][max_a] = 1 - epsilon / A * (A - 1)\n",
    "\n",
    "        episode_history.append((state, a, reward))\n",
    "        state = next_state\n",
    "\n",
    "        if next_state[0] == r_target and next_state[1] == c_target:\n",
    "            break\n",
    "\n",
    "    # 3. n-step Sarsa. Coner cases.\n",
    "    for sarsa_n_corner in range(1, sarsa_n):\n",
    "        if len(episode_history) <= sarsa_n_corner - 1:\n",
    "            continue\n",
    "        head_idx = -(sarsa_n_corner)\n",
    "        (r_head, c_head), a_head, reward_head = episode_history[head_idx]\n",
    "        TD_target = reward_head\n",
    "        g = gamma\n",
    "        for i in range(1, sarsa_n_corner + 1):\n",
    "            (r_last, c_last), a_last, reward_last = episode_history[head_idx + i]\n",
    "            if i != sarsa_n_corner:\n",
    "                TD_target += g * reward_last\n",
    "            else:\n",
    "                TD_target += g * q_table[state[0] * C + state[1]][a]\n",
    "            g *= gamma\n",
    "        TD_error = q_table[r_head * C + c_head][a_head] - TD_target\n",
    "        q_table[r_head * C + c_head][a_head] -= alpha * TD_error\n",
    "        # Update Policy.\n",
    "        max_a = np.argmax(q_table[r_head * C + c_head])\n",
    "        pi[r_head * C + c_head] = epsilon / A\n",
    "        pi[r_head * C + c_head][max_a] = 1 - epsilon / A * (A - 1)\n",
    "\n",
    "    # Enhance Exploitation.\n",
    "    # if epsilon > 0.001:\n",
    "    #     epsilon -= 0.001\n",
    "\n",
    "    time.sleep(0.2)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    print('len(episode_history):', len(episode_history))\n",
    "    action_grid = np.zeros((R, C))\n",
    "    for r_ in range(R):\n",
    "        for c_ in range(C):\n",
    "            a = np.argmax(pi[r_ * C + c_])\n",
    "            action_grid[r_][c_] = a\n",
    "    print(f'{episode}th Action Grid:\\n', action_grid)\n",
    "\n",
    "clear_output(wait=True)\n",
    "\n",
    "action_grid = np.zeros((R, C))\n",
    "for r_ in range(R):\n",
    "    for c_ in range(C):\n",
    "        a = np.argmax(pi[r_ * C + c_])\n",
    "        action_grid[r_][c_] = a\n",
    "print(f'Converged Action Grid:\\n', action_grid, flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 2. 2.]\n",
      " [2. 2. 2. 2. 3.]\n",
      " [2. 1. 2. 1. 2.]\n",
      " [1. 1. 0. 3. 2.]\n",
      " [4. 1. 0. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(action_grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
